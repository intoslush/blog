---
title: 解决组内爆盘问题
description: '装新电脑发现了的磁盘空间问题，原来磁盘一直都很够用，只是一直不知道有更不会去用.'
publishDate: 2026-01-14 20:51:00
tags:
  - Linux
  - Technology
---

# 从 100G 到 3.5T：一次 Ubuntu + CUDA + 磁盘空间“消失”的完整剖析

## 写在前面：这真的只是“磁盘不够用”吗？

一台新的 Ubuntu 机器，CUDA 和 NVIDIA 驱动安装正常，`nvidia-smi` 一切健康。但当我准备开始真正的深度学习工作时，却发现一个明显不合理的现象：

```bash
df -h
```

根分区 `/` 只有 **100G**。

而我非常确定，这台机器不止这么点空间。
于是，一个看似“新手”的问题出现了：**磁盘空间去哪了？**

但随着排查的深入，我逐渐意识到，这并不是一个简单的配置失误，而是一次**非常典型的 Linux 存储抽象层级错觉**。也正是这次经历，让我重新系统性地梳理了一遍 Linux 的磁盘与文件系统模型。

这篇文章不是为了教你“敲哪几条命令”，而是想回答一个更重要的问题：

> **当系统告诉你“空间不见了”，你该如何从机制层面判断问题出在哪里？**

---

## 一、第一次错觉：`df -h` 并不等于磁盘真实容量

很多人（包括我自己）在第一时间都会做同一件事：
反复运行 `df -h`。

但 `df` 实际回答的问题是：

> **当前已经挂载的文件系统，有多少可用空间？**

注意几个关键词：

* 已挂载
* 文件系统

`df` **完全看不到**以下内容：

* 尚未分区的磁盘空间
* 已分区但未挂载的分区
* LVM 中尚未分配给逻辑卷的空间

换句话说，`df -h` 看到的从来都不是“磁盘有多大”，而只是：

> **“你现在能用到多少”**

这也是为什么，在很多“空间消失”的问题中，`df` 只会不断强化你的错觉，而不是帮助你定位问题。

---

## 二、Linux 存储栈的四层结构（核心认知）

要理解问题真正出在哪里，必须跳出单一命令视角，转而从**分层结构**理解 Linux 的存储体系。

### 1️⃣ 物理设备层（Block Device）

这是最底层：

```bash
lsblk
```

你看到的 `/dev/sda`、`/dev/nvme0n1`，只是**物理或虚拟磁盘的存在性**。

这一层只回答一个问题：

> **系统是否识别了这块盘？**

---

### 2️⃣ 分区层（Partition Table）

分区表（GPT / MBR）决定了：

* 哪些磁盘区域是“可分配的”
* 哪些区域被切成了分区

通过：

```bash
parted /dev/sda print
```

你会看到一个很常见但容易忽略的事实：

> **磁盘大小 ≠ 分区大小**

磁盘是 960G，但分区可能只到 300G，剩余空间依然是“空的”。

---

### 3️⃣ LVM 层（最容易制造错觉）

如果你使用了 LVM（Ubuntu 默认安装常见），事情会再复杂一层：

* Physical Volume (PV)
* Volume Group (VG)
* Logical Volume (LV)

一个非常典型的情况是：

* 整块盘已经加入 VG
* 但只给 `/` 分了 100G 的 LV
* 剩余空间全部留在 VG 的 free extents 中

**空间真实存在，但不可见、不可用。**

这正是我这次遇到的核心问题。

---

### 4️⃣ 文件系统与挂载层

只有当一个 LV / 分区：

* 格式化成文件系统
* 被挂载到某个目录

它才会出现在：

```bash
df -h
```

也就是说：

> **`df` 是最上层的“结果视图”，不是诊断工具。**

---

## 三、Ubuntu 的一个经典设计选择：安全，但不一定合理

很多人会问一个问题：

> 为什么 Ubuntu 要默认只给 `/` 100G？

答案并不复杂：

* 面向多用户环境
* 面向服务器场景
* 为未来扩展 `/home`、`/data` 留空间

这是一个**偏运维安全的默认策略**。

但问题在于：

> **这个策略与单用户科研 / 深度学习工作站严重不匹配。**

在科研环境中：

* conda / pip / CUDA / Docker 都在 `/`
* cache、编译中间文件增长极快
* 用户很少主动管理 LVM

结果就是：

* 表面上“系统正常”
* 实际上频繁踩满盘、报奇怪错误

---

## 四、第二个常见疑问：分区变大，会不会变慢？

在我决定把 `/` 从 100G 扩到 3.5T 时，一个合理但错误的直觉出现了：

> “文件系统变这么大，会不会拖慢系统？”

答案是明确的：

**不会。**

原因在于 ext4 的设计：

* 基于 block group 的空间管理
* extent-based allocation
* 目录索引使用 HTree（近似 B-tree）

文件系统的访问复杂度，主要与：

* 文件数量
* 目录结构
* I/O 模式

有关，而**几乎不与总容量线性相关**。

更重要的是：

> 扩容反而降低了“接近满盘”导致的碎片化与性能退化风险。

---

## 五、一次“空间消失”问题的正确诊断路径

回顾整个过程，我总结出一条非常清晰、可复用的排查路径：

1. `lsblk`
   → 确认物理设备是否存在

2. `parted` / `fdisk`
   → 确认分区是否覆盖整盘

3. `vgdisplay` / `lvdisplay`（如使用 LVM）
   → 确认空间是否卡在 VG 层

4. `mount` / `df`
   → 最后才看文件系统视角

一个简单但极其有用的判断原则是：

> **空间“不见了”，一定是在某一层“断开”了。**

---

## 六、针对深度学习工作站的理想磁盘布局

结合这次经历，一个非常实用的布局建议是：

* `/`（大容量）

  * CUDA
  * conda
  * pip / torch / huggingface cache
  * Docker

* `/data` 或独立数据盘

  * datasets
  * checkpoints
  * 长期实验结果

这比“100G `/` + 什么都往数据盘塞”要稳定得多。

---

## 七、这次经历带来的三个长期教训

1. **工具输出不是事实本身**
   `df`、`nvidia-smi` 都是“上下文相关”的视图。

2. **默认配置不等于合理配置**
   特别是在科研、单用户场景中。

3. **系统级心智模型，比记命令重要得多**

---

## 结语

这次问题的解决，本身并不复杂。
但它暴露的，却是一个非常容易被忽视的事实：

> **很多“诡异的系统问题”，本质上只是我们对系统抽象层级理解不够清晰。**

如果这篇文章能帮你在下次看到“磁盘空间消失”时，少走一点弯路，那它的目的就达到了。


